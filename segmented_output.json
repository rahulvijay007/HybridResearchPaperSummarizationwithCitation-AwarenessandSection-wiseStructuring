{
  "unknown": "CitationSum: Citation-aware Graph Contrastive Learning for Scientific Paper Summarization Zheheng Luo zheheng.luo@postgrad.manchester.ac.uk University of Manchester Manchester, UK Qianqian Xie\u2217 qianqian.xie@manchester.ac.uk University of Manchester Manchester, UK Sophia Ananiadou Sophia.Ananiadou@manchester.ac.uk University of Manchester Manchester, UK",
  "abstract": "0.2391 0.0443 0.0267 0.0001",
  "introduction": "0.3353 0.0689 0.2042 0.03724",
  "references": "[1] Amjad Abu-Jbara and Dragomir Radev. 2011. Coherent citation-based summa- rization of scientific papers. In Proceedings of the 49th annual meeting of the association for computational linguistics: Human language technologies. 500\u2013509. [2] Titipat Achakulvisut, Daniel Acuna, and Konrad Kording. 2020. Pubmed Parser: A Python Parser for PubMed Open-Access XML Subset and MEDLINE XML Dataset XML Dataset. Journal of Open Source Software 5, 46 (2020), 1979. https: //doi.org/10.21105/joss.01979 [3] Chenxin An, Ming Zhong, Yiran Chen, Danqing Wang, Xipeng Qiu, and Xuanjing Huang. 2021. Enhancing scientific papers summarization with citation graph. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 35. 12498\u201312506. [4] David M Blei, Andrew Y Ng, and Michael I Jordan. 2003. Latent dirichlet allocation. Journal of machine Learning research 3, Jan (2003), 993\u20131022. [5] Shuyang Cao and Lu Wang. 2021. CLIFF: Contrastive Learning for Improving Faithfulness and Factuality in Abstractive Summarization. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing. 6633\u20136649. [6] Arman Cohan and Nazli Goharian. 2015. Scientific Article Summarization Using Citation-Context and Article\u2019s Discourse Structure. In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing. 390\u2013400. [7] G\u00fcnes Erkan and Dragomir R Radev. 2004. Lexrank: Graph-based lexical centrality as salience in text summarization. Journal of artificial intelligence research 22 (2004), 457\u2013479. [8] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. 2021. Domain-specific language model pretraining for biomedical natural language processing. ACM Transactions on Computing for Healthcare (HEALTH) 3, 1 (2021), 1\u201323. [9] Frank Hansen and Gert K Pedersen. 2003. Jensen\u2019s operator inequality. Bulletin of the London Mathematical Society 35, 4 (2003), 553\u2013564. [10] Jinpeng Hu, Zhuo Li, Zhihong Chen, Zhen Li, Xiang Wan, and Tsung-Hui Chang. 2022. Graph Enhanced Contrastive Learning for Radiology Findings Summariza- tion. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 4677\u20134688. [11] Jacob Devlin Kenton, Ming-Wei Chang, Toutanova, and Lee Kristina. 2019. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In Proceedings of NAACL-HLT. 4171\u20134186. [12] Daniel Kershaw and Rob Koeling. 2022. Sequence-Based Extractive Summarisa- tion for Scientific Articles. In Companion Proceedings of the Web Conference 2022 (Virtual Event, Lyon, France) (WWW \u201922). Association for Computing Machinery, New York, NY, USA, 751\u2013757. https://doi.org/10.1145/3487553.3524652 [13] Madian Khabsa and C. Lee Giles. 2014. The Number of Scholarly Documents on the Public Web. PLOS ONE 9, 5 (05 2014), 1\u20136. https://doi.org/10.1371/journal. pone.0093949 [14] Thomas N Kipf and Max Welling. 2016. Variational Graph Auto-Encoders. stat 1050 (2016), 21. [15] Omer Levy and Yoav Goldberg. 2014. Neural word embedding as implicit matrix factorization. Advances in neural information processing systems 27 (2014). [16] Mike Lewis, Yinhan Liu, Naman Goyal, Marjan Ghazvininejad, Abdelrahman Mohamed, Omer Levy, Veselin Stoyanov, and Luke Zettlemoyer. 2020. BART: Denoising Sequence-to-Sequence Pre-training for Natural Language Generation, Translation, and Comprehension. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 7871\u20137880. [17] Chin-Yew Lin and Eduard Hovy. 2003. Automatic evaluation of summaries using n-gram co-occurrence statistics. In Proceedings of the 2003 human lan- guage technology conference of the North American chapter of the association for computational linguistics. 150\u2013157. [18] Junpeng Liu, Yanyan Zou, Hainan Zhang, Hongshen Chen, Zhuoye Ding, Caixia Yuan, and Xiaojie Wang. 2021. Topic-Aware Contrastive Learning for Abstrac- tive Dialogue Summarization. In Findings of the Association for Computational Linguistics: EMNLP 2021. 1229\u20131243. [19] Yang Liu and Mirella Lapata. 2019. Text Summarization with Pretrained Encoders. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Pro- cessing (EMNLP-IJCNLP). 3730\u20133740. [20] Yixin Liu and Pengfei Liu. 2021. SimCLS: A Simple Framework for Contrastive Learning of Abstractive Summarization. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 2: Short Papers). 1065\u20131072. [21] Zheheng Luo, Qianqian Xie, and Sophia Ananiadou. 2022. Readability Con- trollable Biomedical Document Summarization. arXiv preprint arXiv:2210.04705 (2022). [22] Rada Mihalcea and Paul Tarau. 2004. Textrank: Bringing order into text. In Pro- ceedings of the 2004 conference on empirical methods in natural language processing. 404\u2013411. [23] Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems 26 (2013). [24] Ramesh Nallapati, Feifei Zhai, and Bowen Zhou. 2017. Summarunner: A recurrent neural network based sequence model for extractive summarization of documents. In Thirty-First AAAI Conference on Artificial Intelligence. [25] Feng Nan, Cicero dos Santos, Henghui Zhu, Patrick Ng, Kathleen Mckeown, Ramesh Nallapati, Dejiao Zhang, Zhiguo Wang, Andrew O Arnold, and Bing Xiang. 2021. Improving Factual Consistency of Abstractive Summarization via Question Answering. In Proceedings of the 59th Annual Meeting of the Associa- tion for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers). 6881\u20136894. [26] Vahed Qazvinian and Dragomir Radev. 2008. Scientific Paper Summarization Using Citation Summary Networks. In Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008). 689\u2013696. [27] Amit Sarkar and G. Srinivasaraghavan. 2018. Contextual Web Summarization: A Supervised Ranking Approach. In Companion Proceedings of the The Web Conference 2018 (Lyon, France) (WWW \u201918). International World Wide Web Conferences Steering Committee, Republic and Canton of Geneva, CHE, 105\u2013106. https://doi.org/10.1145/3184558.3186951 [28] Abigail See, Peter J Liu, and Christopher D Manning. 2017. Get To The Point: Summarization with Pointer-Generator Networks. In Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 1073\u20131083. [29] Ilya Sutskever, Oriol Vinyals, and Quoc V Le. 2014. Sequence to sequence learning with neural networks. Advances in neural information processing systems 27 (2014). [30] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, \u0141ukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Advances in neural information processing systems 30 (2017). [31] Benyou Wang, Qianqian Xie, Jiahuan Pei, Prayag Tiwari, Zhao Li, et al. 2021. Pre-trained language models in biomedical domain: A systematic survey. arXiv preprint arXiv:2110.05006 (2021). [32] Danqing Wang, Jiaze Chen, Hao Zhou, Xipeng Qiu, and Lei Li. 2021. Contrastive Aligned Joint Learning for Multilingual Summarization. In Findings of the Associ- ation for Computational Linguistics: ACL-IJCNLP 2021. 2739\u20132750. [33] Max Welling and Thomas N Kipf. 2016. Semi-supervised classification with graph convolutional networks. In J. International Conference on Learning Representations (ICLR 2017). [34] Qianqian Xie, Jennifer Amy Bishop, Prayag Tiwari, and Sophia Ananiadou. 2022. Pre-trained language models with domain knowledge for biomedical extractive summarization. Knowledge-Based Systems 252 (2022), 109460. [35] Qianqian Xie, Jimin Huang, Pan Du, and Min Peng. 2021. Graph Relational Topic Model with Higher-order Graph Attention Auto-encoders. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021. 2604\u20132613. [36] Qianqian Xie, Jimin Huang, Tulika Saha, and Sophia Ananiadou. 2022. GRE- TEL: Graph Contrastive Topic Enhanced Language Model for Long Document Extractive Summarization. In Proceedings of the 29th International Conference on Computational Linguistics. 6259\u20136269. [37] Qianqian Xie, Yutao Zhu, Jimin Huang, Pan Du, and Jian-Yun Nie. 2021. Graph neural collaborative topic model for citation recommendation. ACM Transactions on Information Systems (TOIS) 40, 3 (2021), 1\u201330. [38] Michihiro Yasunaga, Jungo Kasai, Rui Zhang, Alexander R Fabbri, Irene Li, Dan Friedman, and Dragomir R Radev. 2019. Scisummnet: A large annotated corpus and content-impact models for scientific paper summarization with citation networks. In Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33. 7386\u20137393. [39] Chrysoula Zerva, Minh-Quoc Nghiem, Nhung TH Nguyen, and Sophia Anani- adou. 2020. Cited text span identification for scientific summarisation using pre-trained encoders. Scientometrics 125, 3 (2020), 3109\u20133137. A SUMMARISATION EXAMPLES WWW \u201923, May 1\u20135, 2023, Austin, TX, USA Trovato and Tobin, et al. Table 8: An example of generated summary by our method on the PubMedCite dataset. The gold summary is the abstract of the document. Gold: Recent work has provided new insights into the mechanism of spindle assembly. Growing evidence supports a model in which the small GTPase Ran plays a central role in this process. Here, we examine the evidence for the existence of a RanGTP gradient around mitotic chromosomes and some controversial data on the role that chromosomes play in spindle assembly. We review the current knowledge on the Ran downstream targets for spindle assembly and we focus on the multiple roles of TPX2, one of the targets of RanGTP during cell division. Generated: Recent studies have shown that the targeting protein for xklp2 ( tpx2 ), one of the central targets of ran in spindle assembly, is a central target of ran. In this review , we examine some controversial data on the role of chromosomes and the ran model in spindle formation. We then review our current knowledge on the downstream targets of rangtp during mitosis and we focus on the multiple functions performed by tpx2 in mitosis. Finally, we discuss the functions of tpx2 in the regulation of spindle assembly and discuss the potential of tpx2 as a target for ran in mitosis and in other cell cycle regulated processes. Selected content of reference: While this is an attractive model, it is likely that the true situation is more complicated and that the location of other Ran pathway components can play a role in regulating Ran-GTP distribution. Ran is a GTPase that is required for nuclear transport, cell cycle control, mitotic spindle formation, and postmitotic nuclear assembly, although the mechanism of this targeting has not been reported. We wished to determine whether SUMO-1 plays a role in RanGAP1\u2019s targeting to the spindle during mitosis. We wished to determine more precisely where RanGAP1 localized on the spindle. The small GTPase Ran plays a key role in diverse cellular functions including nucleocytoplasmic transport, nuclear envelope formation, and spindle assembly. This unique localization of the Ran regulators strongly suggests that there is a RanGTP concentration gradient across the interphase nuclear envelope and on the condensed mitotic chromosomes. (Mean ROUGE: 0.519) Abstract of reference: The microtubule cytoskeleton plays a pivotal role in cytoplasmic organization, cell division, and the correct transmission of genetic information. In a screen designed to identify fission yeast genes required for chromosome segregation, we identified a strain that carries a point mutation in the SpRan GTPase. Ran is an evolutionarily conserved eukaryotic GTPase that directly participates in nucleocytoplasmic transport and whose loss affects many biological processes. Recently a transport-independent effect of Ran on spindle formation in vitro was demonstrated, but the in vivo relevance of these findings was unclear. Here, we report the characterization of a Schizosaccharomyces pombe Ran GTPase \u00b7 \u00b7 \u00b7 (Mean ROUGE: 0.241)",
  "methodology": "0.4245 0.0801 0.3718 0.0712",
  "experiments": "Datasets. To evaluate the effectiveness of our methods, we con- duct experiments on SSN and PubMedCite datasets, as shown in Table 4. Following the previous method [3], we have both trans- ductive and inductive settings in PubMedCite. For SSN, we use its original train/validation/test data splitting: 128,299/6,250/6,250 for inductive setting, and 128,400/6,123/6,276 for transductive set- ting. Different from it, we keep the same train/validation/test set splitting: 178,100/7,036/7,608 in our PubMedCite in both settings. This makes a more fair comparison of the influence of performance with different training setting, since we only have different citation correlations in inductive and transductive settings. Following pre- vious work, we use the abstracts of scientific documents as target summaries. Baselines. We compare our method with: 1) extractive methods: LEAD [28], a simple method that selects the first n sentences; Tex- tRank [22], a graph-based ranking method; TransformerEXT [19], a transformer encoder-based method; BERTSUMEXT [19], a BERT encoder based method; BERTSUMEXT+ [19], a BERT encoder based method with 640 input tokens, 2) abstractive methods: PTGEN+COV [28], a method based on copy mechanism; TransformerABS [19]: a transformer encoder based method; BERTSUMABS [19], that uses the BERT encoder; BERTSUMABS+ [19], the BERT encoder based method with 640 input tokens; BART [16], a competitive pre-trained language model for text generation; CGSUM [3], the SOTA method for the citation graph summarization task, based on the graph neural network. We report the F1 score of unigram overlapping(ROUGE-1), bigram overlapping(ROUGE-2), and longest common subsequence (ROUGE-L) between generated summaries and gold summaries [17]. Implementation Detail. Our method is implemented by Python and Pytorch2. We use the implementation of BERT, BART and Pub- MedBERT from Huggingface3. We run our experiments on multiple GPUs with 32G memory. Following the previous method [19, 34], we set the different learning rates to PLMs-based encoder and trans- former based decoder. We set the learning rate of the encoder to 2\ud835\udc52\u22123, that of the decoder to 2\ud835\udc52\u22121, the drop-out rate of the decoder to 0.4, the training steps to 200000, the warm-up steps of the en- coder to 20000, that of the decoder to 10000, the maximize a token number of input documents to 1240, and that of each reference of input documents to 100. \ud835\udefc, \ud835\udefdon graph contrastive loss controlling, is set to 1. We save checkpoint at every 200 steps, and select the best checkpoint according to the validation. Due to the memory 2Our model and data can be accessed at here https://github.com/zhehengluoK/ CitationSum 3Models are initialized from checkpoints \"bert-base-uncased\", \"microsoft/BiomedNLP- PubMedBERT-base-uncased-abstract\", and \"facebook/bart-base\" CitationSum: Citation-aware Graph Contrastive Learning for Scientific Paper Summarization WWW \u201923, May 1\u20135, 2023, Austin, TX, USA limitation, we set the maximum number of neighbours of input documents to 16. For each document, we take tokens from its refer- ences as the negative tokens. During the content selection of the test data set, we use the introduction of the source document to select contents from its references. neighbour references of the test may be from the training data in the transductive setting, while the dataset is split into totally independent train/validation/test sets in the inductive setting. 5.1 Results Analysis 5.1.1 Main Results. We first show the ROUGE F1 score of different methods in both datasets in Table 5. We investigate both BERT [11] and PubMedBERT [8] as the encoder in our method. Our method with the PubMedBERT-based encoder (CitationSum + PubMed- BERT) presents the best performance among all baselines on both datasets when evaluating R-1 (ROUGE-1) and R-2 (ROUGE-2) for informativeness and R-L (ROUGE-L) for fluency. When compared with CGSUM which also incorporates citation graphs to enhance summarization generation, it outperforms CGSUM in both induc- tive and transductive settings. Different from CGSUM which uses abstracts of references, our method incorporates high-quality infor- mation from the full contents of references under the guidance of its semantic similarity with the source papers. This proves the advan- tage of our method to make better and full use of references and the structure information of the citation graph. It is also demonstrated by the superior performance of our method using BERT encoder compared with all BERT based abstractive methods including BERT- SUMABS+, BERTSUMABS and BERTSUMABS+Concat Nbr.Summ. This proves essential to capture salient information about refer- ences and varying semantic correlations between scientific scripts and their references. Moreover, although CGSUM is based on the LSTM backbone model, it outperforms pre-trained language model- based methods such as BERTSUMABS and CitationSum + BART. This proves that the performance improvement of our method is not attributed to using the pre-trained language model, but due to make better leveraging the information of the citation graph. Although it has been proven that BART shows better perfor- mance than BERT in text summarization, we surprisingly find that CitationSum + BART doesn\u2019t have the advantage for domain- specific scientific papers when compared with the CitationSum + BERT. We can also observe that CitationSum + BERT underper- forms CitationSum + PubMedBERT in all datasets and CGSUM in PubMedCite due to the limited vocabulary of BERT. Since there are many terminologies in documents of PubMedCite, the PubMedCite dataset is more challenging for BERT based methods compared with SSN. During experiments, we find that when using BERT\u2019s original tokenizer, there would be so many unrecognized tokens in PubMedCite to be set as [UNK] that causes much information loss and leads to many meaningless [UNK] tokens being generated in summaries. In contrast, on SSN our method with BERT encoder out- performs CGSUM and has no tokens unrecognized as [UNK], and CitationSum + PubMedBERT has limited improvement on the per- formance of SSN compared with CitationSum + BERT. This proves that the PubMedCite dataset with high technical domain-specific papers is more challenging when compared with SSN. Moreover, our models and CGSUM both have superior perfor- mance to other models that ignore information of references includ- ing BERT and transformer-based extractive and abstractive meth- ods, as well as traditional methods TextRank and PTGEN+COV. We can notice that simply appending the content from reference papers (Concat Nbr.Summ) in baseline methods presents limited benefit or even yield worse performance. For example, the BERTSUM- ABS+Concat Nbr.Summ with the extra input tokens from abstracts of references seldom underperforms the BERTSUMABS+. This may be because abstracts of references can not provide useful infor- mation or even introduce extra noise as we mentioned before in Section 1. This indicates that although leveraging references is ben- eficial to better understand scientific papers, it is vital to distinguish between salient and non-salient information in references. 5.2 Ablation Study To further clarify the contribution of each component in our method, we perform experiments on our method and several ablations which respectively removes contrastive learning (W/O Contra), document representation alignment (W/O DRA), token representation align- ment (W/O TRA), and concatenation of token representations of references (W/O Concate). As shown in Table 6, the performance of our method suffers least without concatenation, indicating the efficacy of our hierarchical graph contrastive learning in delivering salient information from references to encoding source documents. It can also be demonstrated by the lowest ROUGE score of W/O Contra since without our design of contrastive learning, the model can be faced with difficulty finding useful messages from concate- nated reference content. Moreover, the drops brought by W/O DRA and W/O TRA separately manifest the essential of inter-document and inner-document connections in our design. In the absence of ei- ther of them, information propagation in the heterogeneous graph would be blocked, inhabiting the model\u2019s ability to fully understand the source paper. 5.3 Parameter Impacts We further explore the influence of parameter \ud835\udf0cof controlling edge weights on the performance. \ud835\udf0cis used to filter edges between the source document and its references in the citation graph, according to their semantic similarity as shown in Section 4.2.1. From Table 7, we can see that our model yields the best performance when \ud835\udf0c= 0.7. A too low value of \ud835\udf0c(\ud835\udf0c< 0.7) could incorporate references that have low semantic similarity with the source document. Thus extra noise could be introduced. In contrast, the too high value of \ud835\udf0c(0.7 < \ud835\udf0c) could filter references with useful information. 5.4 Case Study In Appendix A Table 8, we show the generated summary by our method of an example document along with its reference. Tokens that are semantically correlated to the example document, are marked with blue colour. It shows that our model is able to generate a coherent summary for the document that is highly semantically related to its gold summary. Compared with the abstract of its refer- ence, the selected high-quality content of its reference can provide more useful information indicated by a higher mean rouge score with the example document. This is also illustrated in the selected WWW \u201923, May 1\u20135, 2023, Austin, TX, USA Trovato and Tobin, et al. Table 5: ROUGE F1 results of different models on SSN and PubMedCite. The results of our model are under 5 times running. \u2020 means outperform the existing model with best performance significantly (\ud835\udc5d< 0.05). Part results are from [3]. Datasets SSN PubMedCite Setting Inductive Transductive Inductive Transductive Metrics R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L LEAD 28.29 5.99 24.84 28.30 6.87 24.93 28.06 6.38 25.59 29.27 6.75 26.59 TextRank 36.36 9.67 32.72 40.81 12.81 36.47 38.87 11.35 34.42 39.00 11.37 34.50 TransformerEXT 43.14 13.68 38.65 41.45 13.02 37.20 38.46 11.96 35.68 38.41 11.59 35.80 BERTSUMEXT 42.41 13.10 37.97 41.68 13.31 37.42 38.72 11.96 35.86 38.82 11.96 35.98 BERTSUMEXT+ 44.28 14.67 39.77 43.23 14.59 38.91 39.14 12.02 36.18 39.18 12.09 36.22 PTGEN+COV 42.84 13.28 37.59 39.46 12.06 35.72 - - - - - - Concat Nbr.Summ 43.05 13.53 37.97 40.12 12.58 35.94 - - - - - - TransformerABS 37.78 9.59 34.21 36.58 10.19 33.13 34.41 10.59 31.80 33.36 10.37 31.11 +Copy 41.22 13.31 37.22 40.83 14.71 36.93 38.36 11.86 35.55 38.72 11.96 35.86 BERTSUMABS 43.73 15.05 39.46 40.38 14.07 36.54 39.07 11.78 35.33 39.11 11.78 36.38 BERTSUMABS+ 43.73 15.05 39.46 41.92 15.09 37.79 39.15 11.38 35.58 39.22 12.28 36.31 Concat Nbr.Summ 43.45 14.89 39.27 41.11 14.50 37.16 39.13 11.81 35.84 39.38 11.88 36.01 CGSUM 44.28 14.75 39.76 43.45 14.71 38.89 40.52 12.10 37.07 39.91 11.89 36.78 CitationSum+BART - - - - - - 39.37 11.46 35.98 39.43 11.52 36.05 CitationSum+BERT 44.72 15.03 40.12 44.07 15.02 39.47 39.48 12.24 36.47 39.62 12.29 36.54 +PubMedBERT 45.01\u2020 15.18\u2020 40.59\u2020 44.26\u2020 15.45\u2020 39.59\u2020 41.62\u2020 13.29\u2020 37.54\u2020 41.76\u2020 13.36\u2020 37.69\u2020 Table 6: ROUGE F1 results of our model under different settings on SSN and PubMedCite. Datasets SSN PubMedCite Setting Inductive Transductive Inductive Transductive Metrics R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L R-1 R-2 R-L W/O Contra 44.77 14.70 39.66 43.92 14.83 38.84 40.16 12.51 37.13 39.99 12.43 36.93 W/O DRA 44.83 14.91 40.02 44.07 14.88 38.97 40.17 12.58 37.18 40.07 12.52 37.01 W/O TRA 44.94 14.95 40.18 44.15 14.91 39.09 40.90 12.93 37.16 40.08 13.17 37.25 W/O Concate 44.96 15.13 40.57 44.22 15.53 39.37 41.38 13.29 37.51 41.42 13.29 37.69 Full 45.01\u2020 15.18\u2020 40.59\u2020 44.26\u2020 15.45\u2020 39.59\u2020 41.62\u2020 13.29\u2020 37.54\u2020 41.76\u2020 13.36\u2020 37.69\u2020 Table 7: ROUGE F1 results of our model with different values of \ud835\udf0cunder inductive setting, that controls weights of edges in the citation graph. Datasets SSN PubMedCite Metrics R-1 R-2 R-L R-1 R-2 R-L 0.5 44.69 14.72 40.97 41.09 12.54 37.04 0.6 44.94 14.98 40.02 41.20 13.29 37.13 0.7 45.01 15.18 40.59 41.62 13.29 37.54 0.8 44.82 14.88 40.38 41.27 12.95 37.18 content of references which have a large number of related tokens that are marked with blue colour. Our method can effectively rec- ognize salient information from references for fully exploiting the hierarchical connections among documents and tokens, which is utilized to generate concise summaries with many relevant tokens marked in blue colour. 6",
  "related work": "2.1 Scientific Paper Summarization Online scientific document as a web-based text resource, its au- tomatic summarization has attracted much attention[12, 27]. One direction to tackle this problem is citation-assisted summarization, which aims to highlight the main contributions of papers, based on citation sentences from papers that cite the source document. The earliest attempts at citation-based summarisation [1, 6, 26], used the sentence clustering and ranking methods such as Lexrank [7], to select citation sentences of papers, as the summary for the paper referenced by them. In addition to the cited text span from papers, Yasunaga et al. [38] further utilized the abstract of the target paper, and the graph neural networks (GNNs) [33] to encode all input texts. Zerva et al. [39] investigated the advanced pre-trained encoders BERT [11] to identify and select citation text spans. However, these methods cannot address if a newly published paper has not been cited by any paper. To address this gap, An et al. [3] recently pro- posed the citation graph-based summarization task which considers both the contents of target papers and their corresponding refer- ences in the citation graph. However, they only utilized abstracts of references in a shallow manner, which inspires us to fully leverage references and capture the correlations between source papers and their references, via graph contrastive learning. 2.2 Text Summarization with Contrastive Learning Recently, contrastive learning has been introduced to improve text summarization. Liu and Liu [20] proposed to use the contrastive learning to optimize the quality of generated summaries accord- ing to the evaluation metric ROUGE. Cao and Wang [5] and Nan et al. [25] utilized the contrastive learning to improve the factuality of abstractive summarization. Liu et al. [18] proposed the topic- aware contrastive loss to capture dialogue topic information for abstractive dialogue summarization. Wang et al. [32] designed the contrastive loss to align sentence representation across different languages, for multilingual summarization. Hu et al. [10] used the contrastive learning to improve the graph encoder, for the radiol- ogy findings summarization. Different from them, we focus on the citation graph-based summarization task, which has been rarely studied. Xie et al. [36] designed the graph contrastive learning to capture the better topic information for long document summariza- tion. 3 CITATION-AWARE PUBMED DATASET To support the evaluation and development of our method, we first introduce a new large-scale citation-aware PubMed dataset (Pub- MedCite) with 192,744 scientific paper nodes and 917,838 citation relationships in the biomedical domain, that are extracted from the PubMed Central Open Access Subset 1. The PubMedCite corpus is built on the PubMed Central Open Access Subset. To construct PubMedCite, we first downloaded the whole PubMed Open Access Subset (up to 17 Nov. 2021), then build the graph by adding papers into it through the breadth first traversal starting from a random document until the number of nodes reaches a limit. During the construction, we utilize pubmed parser by Achakulvisut et al. [2] to extract the PMC id, pubmed id, title, abstract, and full article of each document. For the inductive setting, we used the same graph building method to sample two different sub-graphs from the whole PubMedCite citation graph as the validation and test set. Then, we removed the inter-graph edges among the three sets to ensure their independence. Although it is not for commercial use, we are unable to release document contents of the dataset directly due to license limitations (such as CC BY-SA, and CC BY-ND licenses) of some papers. We will release the build citation graph among all docu- ments, and provide the code script for users to access and process the document contents themselves, according to the paper id saved in the citation graph. The statistics of the PubMedCite dataset and comparison with the only existing dataset SSN [3] are shown in Table 4. When compared with SSN: 1) The average length of gold summary in our dataset is longer than that of SSN, while the average length of full articles is relatively shorter than that of SSN. 2) Our dataset keeps full contents of references to make better use of their information, while SSN only keeps abstracts of them. 3) Our dataset only includes papers in the biomedical domain, while SSN consists of papers from several different fields including mathematical, physics, and computer science, our dataset can help the evaluation of domain- specific tasks in the research community. Moreover, biomedical scientific papers are laden with terminology and have complex syntactic structures [21, 31]. This makes our dataset a challenging benchmark for automatic summarization methods. 4",
  "methods": "We first define the task of scientific paper summarization with the citation graph. Given a corpus \ud835\udc37, each document \ud835\udc51in the corpus is represented by the sequence of \ud835\udc5btokens: \ud835\udc65= {\ud835\udc651,\ud835\udc652, \u00b7 \u00b7 \u00b7 ,\ud835\udc65\ud835\udc5b}. Its target summary is represented by a sequence of \ud835\udc5atokens: \ud835\udc66= {\ud835\udc661,\ud835\udc652, \u00b7 \u00b7 \u00b7 ,\ud835\udc66\ud835\udc5a}, where \ud835\udc5a\u226a\ud835\udc5b. The citation graph \ud835\udc3a= {\ud835\udc49, \ud835\udc38} of the corpus preserves citation relationships among all documents, where\ud835\udc49is the set of document nodes and \ud835\udc38is the set of edges. \ud835\udc38can be represented by the adjacency matrix \ud835\udc34, where \ud835\udc34\ud835\udc51,\ud835\udc51\u2032 = 1 means there exists a citation link between document \ud835\udc51and \ud835\udc51\u2032, \ud835\udc34\ud835\udc51,\ud835\udc51\u2032 = 0 otherwise. For each source document\ud835\udc51, it aims to generate its target summary\ud835\udc66based on the source paper and the sub-citation graph \ud835\udc34\ud835\udc51 with only the source document and all its neighbours. It is generally considered as a conditioned sequence-to-sequence [29] learning problem to model the generation process \ud835\udc5d(\ud835\udc66|\ud835\udc65,\ud835\udc34\ud835\udc51). In this section, we will introduce our proposed citation-aware scientific paper summarization framework (CitationSum) based on the graph contrastive learning and the citation graph. CitationSum aims to fully leverage the useful information of references and the weighted citation correlation in the citation graph. Different from previous methods which used only the abstracts of references [3], 1https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/ WWW \u201923, May 1\u20135, 2023, Austin, TX, USA Trovato and Tobin, et al. Table 4: The statistics. \"Sum words\" and \"Sum sent\" denote the average word and sent number of summarizations. Dataset Train Dev Test Word Sent Sum words Sum sent Edges Domain SSN 128,400 6,123 6,276 5072.3 290.6 165.1 6.4 660,908 General Our 178,100 7,036 7,608 4409.7 159.8 265.1 10.1 917,838 Biomedical Figure 1: The model architecture. we first select the key information from the full contents of ref- erences for each source document and build its weighted citation graph to capture the varying semantic correlation between the source paper and its references. To further make the deep informa- tion fusing between the source paper and its references, we build the hierarchical heterogeneous graph based on the weighted citation graph and document contents, to capture the semantic correlations between the source paper, references, and their tokens. As shown in Figure 1, we encode the source document and its references with the PLMs-based encoder to yield token-level representations. We use the pooling strategy to further generate document representations for the source paper and its references, based on the token represen- tations. The self-supervised graph contrastive learning is designed to align representations of the document and references at both document and token levels, through the hierarchical heterogeneous graph. This allows the model to integrate the salient information of references into the source paper according to their varying se- mantic similarity, to improve the summarization generation of the source paper. Finally, the citation-aware token representations of the source paper are fed into the decoder to conduct the summa- rization generation. 4.1 Input Representation Contents Selection As shown in Table 3, sentences that are rele- vant to the source paper are found in the full contents of references. Unlike previous methods considering only abstracts [3], we first aim to identify the most useful contents of references to make bet- ter use of them. Similar to the oracle summary selection, for each source document, we use a greedy selection algorithm [19, 24] to extract the top sentences from the full contents of the references. This process maximises the ROUGE score against the target sum- mary of the source document (generally the abstract). This heuristic approach iteratively selects one sentence adding to the summary til the ROUGE score cannot be improved by adding any more to the summary. It is noticed that we use the introduction of the source document instead of the target summary to make the content se- lection for the test set since the target summary of the source document should be unseen during the test process. As shown in Table 3, the abstract and introduction of the source paper have comparable semantic similarity with references, demonstrating the necessity of extracting key contents from introductions. We also narrow down the sub-graph by only considering the most relevant neighbour nodes from the full citation graph \ud835\udc34\ud835\udc51for each document \ud835\udc51, following the previous method [3]. To reduce the computational costs of processing all references and encoding the full graph, we sample the sub-graph with the top neighbour references from the full citation graph \ud835\udc34\ud835\udc51for each document \ud835\udc51. Our method differs from An et al. [3] that extracted neighbours based on their hidden representations, we select references that have the maximum semantic similarity (measured by the ROUGE score) with the source document from neighbour references. Input Encoder. Given the source document \ud835\udc51by the sequence of tokens {\ud835\udc651,\ud835\udc652, \u00b7 \u00b7 \u00b7 ,\ud835\udc65\ud835\udc5b}, and its references \ud835\udc51\u2032 \u2208N\ud835\udc51in the sub citation graph \ud835\udc34\u2032 \ud835\udc51, we first convert each token of them into the sum of token embedding, position embedding and segmentation embedding. We then yield contextual representations of tokens with the pre-trained language model: \u210e\ud835\udc51{\u2217} = {\u210e\ud835\udc51,1,\u210e\ud835\udc51,2, \u00b7 \u00b7 \u00b7 ,\u210e\ud835\udc51,\ud835\udc5b} = \ud835\udc3f\ud835\udc40({\ud835\udc651,\ud835\udc652, \u00b7 \u00b7 \u00b7 ,\ud835\udc65\ud835\udc5b}). Finally, we aggregate token representations with the pooling layer to achieve the contextual representation of source document: \u210e\ud835\udc51= \ud835\udc39\ud835\udc39\ud835\udc41([\ud835\udc5a\ud835\udc4e\ud835\udc65(\u210e\ud835\udc51{\u2217}),\ud835\udc5a\ud835\udc52\ud835\udc4e\ud835\udc5b(\u210e\ud835\udc51{\u2217})]), where \ud835\udc39\ud835\udc39\ud835\udc41is the feed-forward network. Under the same process, we yield the contextual representation of its references \u210e\ud835\udc51\u2032(\ud835\udc51\u2032 \u2208N\ud835\udc51), where N\ud835\udc51is the set of neighbours of \ud835\udc51in the sub-citation graph. It is noticed that the source paper and its references are input individually into PLMs. 4.2 Citation-aware Graph Contrastive Learning We leverage information from references and the correlation struc- ture in the citation graph to guide summary generation by propos- ing the self-supervised citation-aware graph contrastive learning framework. This enables a rich information fusion between source documents and their references. CitationSum: Citation-aware Graph Contrastive Learning for Scientific Paper Summarization WWW \u201923, May 1\u20135, 2023, Austin, TX, USA 4.2.1 Hierarchical Graph Construction. For each source document, we first build a hierarchical heterogeneous graph with multi-granularity nodes. It consists of a two-level graph organized hierarchically: the weighted citation graph to capture the citation correlations between the source document and its references, and the document graph to model the correlation between documents and tokens. Weighted Citation Graph Construction. For each source doc- ument \ud835\udc51, and all its selected k-hop neighbour references in its sub- citation graph, we build weighted edges for them according to their semantic similarity. We also randomly select documents that have no citation relation with the source document, as the negative nodes in the graph. The edge of two nodes (\ud835\udc56, \ud835\udc57) is defined as: \ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57= \uf8f1\uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f2 \uf8f4\uf8f4\uf8f4\uf8f4\uf8f4\uf8f3 \ud835\udc45\ud835\udc42\ud835\udc48\ud835\udc3a\ud835\udc38(\ud835\udc34\ud835\udc4f\ud835\udc60\ud835\udc61\ud835\udc56,\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc57), \ud835\udc56= \ud835\udc51, \ud835\udc57\u2208N\ud835\udc51 \ud835\udc45\ud835\udc42\ud835\udc48\ud835\udc3a\ud835\udc38(\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc56,\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc57), \ud835\udc57\u2208N\ud835\udc51 1, \ud835\udc56= \ud835\udc57 0, \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 (1) where \ud835\udc45\ud835\udc42\ud835\udc48\ud835\udc3a\ud835\udc38(\ud835\udc34\ud835\udc4f\ud835\udc60\ud835\udc61\ud835\udc56,\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc57) is the mean score of ROUGE-1 and ROUGE-2 between the abstract of the source document and key contents of its neighbours, \ud835\udc45\ud835\udc42\ud835\udc48\ud835\udc3a\ud835\udc38(\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc56,\ud835\udc36\ud835\udc5c\ud835\udc5b\ud835\udc61\ud835\udc57) is the mean score of ROUGE-1 and ROUGE-2 between highlight contents of neigh- bour \ud835\udc56and \ud835\udc57. Edges with weights \ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57that are less than \ud835\udf0c, will be deleted to avoid introducing noise. Bipartite Document Graph Construction. We build the bi- partite graph for each document\ud835\udc51(including source and references), whose nodes are documents and tokens, and edges are occurrences of tokens at the document. We also randomly select tokens from other documents as the negative token nodes in the graph. The edge of two nodes (\ud835\udc56, \ud835\udc57) is defined as: \ud835\udc35\ud835\udc51 \ud835\udc56,\ud835\udc57= ( 1, \ud835\udc56= \ud835\udc51, \ud835\udc57\u2208{\ud835\udc651,\ud835\udc652, \u00b7 \u00b7 \u00b7 ,\ud835\udc65\ud835\udc5b} 0, \ud835\udc5c\ud835\udc61\u210e\ud835\udc52\ud835\udc5f\ud835\udc64\ud835\udc56\ud835\udc60\ud835\udc52 (2) 4.2.2 Graph Contrastive Learning. Document Representation Alignment. Based on the weighted citation graph, we design the following document representation alignment (DRA) loss: L\ud835\udc37\ud835\udc45\ud835\udc34= \u22121 \ud835\udc41\ud835\udc51 \ud835\udc41\ud835\udc51 \u2211\ufe01 \ud835\udc51=1 \ud835\udc59\ud835\udc5c\ud835\udc54( \u00cd 0<\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57\u2212\u02c6\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57\ud835\udc52\u210e\ud835\udc56\u00b7\u210e\ud835\udc57 \u00cd \ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57=0 \ud835\udc52\u210e\ud835\udc56\u00b7\u210e\ud835\udc57 ) (3) where \u02c6\ud835\udc34\u2032\ud835\udc51= \ud835\udc3c\u2212\ud835\udc37\u22121 2 \ud835\udc34\u2032\ud835\udc51\ud835\udc37\u22121 2 is the normalized graph Laplacian of \ud835\udc34\u2032\ud835\udc51, \ud835\udc37is the degree matrix of \ud835\udc34\u2032\ud835\udc51, \ud835\udc41\ud835\udc51is the number of documents, and \u210e\ud835\udc56,\u210e\ud835\udc57are document contextual representations from the PLM encoder. It encourages representations of documents (including the source and its references), and their neighbours to be closer, while pulling away representations of documents that don\u2019t have cita- tion correlation. This makes explicitly information fusion between source documents and key contents of their references according to their semantic correlation in the citation graph, to yield citation- aware document representations for source documents and their neighbours. Token Representation Alignment. To further make informa- tion propagation between token representations of source docu- ments and their neighbours, we then design the document graph- guided contrastive learning, to align token representations with citation aware-document representations. We design the following token representation alignment (TRA) loss based on the bipartite graph: L\ud835\udc47\ud835\udc45\ud835\udc34= \u22121 \ud835\udc41\ud835\udc51 \ud835\udc41\ud835\udc51 \u2211\ufe01 \ud835\udc51=1 \ud835\udc59\ud835\udc5c\ud835\udc54( \u00cd 0<\ud835\udc35\ud835\udc51 \ud835\udc51,\ud835\udc57\u2212\u02c6\ud835\udc35\ud835\udc51 \ud835\udc51,\ud835\udc57\ud835\udc52\u210e\ud835\udc51\u00b7\u210e\ud835\udc57 \u00cd \ud835\udc35\ud835\udc51 \ud835\udc51,\ud835\udc57=0 \ud835\udc52\u210e\ud835\udc51\u00b7\u210e\ud835\udc57 ) (4) where \u02c6\ud835\udc35\ud835\udc51= \ud835\udc3c\u2212\ud835\udc37\u22121 2 \ud835\udc35\ud835\udc51\ud835\udc37\u22121 2 is the normalized graph Laplacian of \ud835\udc35\ud835\udc51, and \u210e\ud835\udc51,\u210e\ud835\udc57are document and token representations from the PLM encoder. It pushes the citation-aware representation of the document and representations of tokens closer if these tokens appeared in the document, and pulls away otherwise. The alignment makes the information of references be propa- gated into token representations of source documents, and also token representations of references be grounded by representations of source documents. 4.3 Graph Contrastive as Matrix Factorizing In this section, we aim to make a theoretical understanding of the information captured by the hierarchical graph contrastive learning process in the above. We can find that the hierarchical graph contrastive learning based on the weighted citation graph and document graph, can be reformulated as the implicit matrix factorization, to reconstruct them. For the equation 4, we derive its upper bound as: L\ud835\udc47\ud835\udc45\ud835\udc34\u2264\u22121 \ud835\udc41\ud835\udc51 \ud835\udc41\ud835\udc51 \u2211\ufe01 \ud835\udc51=1 \ud835\udc59\ud835\udc5c\ud835\udc54( \u00cd 0<\ud835\udc35\ud835\udc51 \ud835\udc51,\ud835\udc57\ud835\udc52\u210e\ud835\udc51\u00b7\u210e\ud835\udc57 \u00cd\ud835\udc41\ud835\udc61 \ud835\udc5c=1 \ud835\udc52\u210e\ud835\udc51\u00b7\u210e\ud835\udc5c ) (5) where \ud835\udc41\ud835\udc61is the number of tokens (including positive and negative tokens). Since \u2212\u02c6\ud835\udc35\ud835\udc51 \ud835\udc51,\ud835\udc57= \ud835\udc35\ud835\udc51 \ud835\udc51,\ud835\udc57 \u221a|\ud835\udc41|\ud835\udc51\u00b7|\ud835\udc41|\ud835\udc57= 1 \u221a\ud835\udc41\ud835\udc61is the same value for all positive token \ud835\udc57, thus can be dependent from the log function and collapsed to a constant, where |\ud835\udc41|\ud835\udc51= \ud835\udc41\ud835\udc61, |\ud835\udc41|\ud835\udc57are the degree of document node \ud835\udc51, and token node \ud835\udc57. To minimize the L\ud835\udc47\ud835\udc45\ud835\udc34, we can instead optimize its upper bound. Following previous methods [15, 23], the upper bound can be ap- proximated with the negative sampling: max 1 \ud835\udc41\ud835\udc51 \ud835\udc41\ud835\udc51 \u2211\ufe01 \ud835\udc56=1 \u2211\ufe01 \ud835\udc57\u2208N+ \ud835\udc51 \ud835\udc5b\ud835\udc51,\ud835\udc57\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udf0e(\u210e\ud835\udc51\u00b7 \u210e\ud835\udc57))+ \ud835\udc58\u00b7 E\ud835\udc5c\u2208N\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udf0e(\u2212\u210e\ud835\udc51\u00b7 \u210e\ud835\udc5c) (6) where \ud835\udc5b\ud835\udc51,\ud835\udc57is the appearance frequency of token \ud835\udc57in document \ud835\udc51, N+ \ud835\udc51is the set of positive tokens appeared in document \ud835\udc51, \ud835\udc58is the number of sampled negative tokens, \ud835\udf0e(\ud835\udc65) = 1 1+\ud835\udc52\u2212\ud835\udc65, and N is the set of tokens in the corpus. For simplify the analysis, we consider the negative tokens are sampled from the empirical unigram distri- bution, thus the expectation term in equation 6 can be rewritten as: E\ud835\udc5c\u2208N\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udf0e(\u2212\u210e\ud835\udc51\u00b7 \u210e\ud835\udc5c) = \u2211\ufe01 \ud835\udc57\u2208N+ \ud835\udc51 \ud835\udc5b\ud835\udc57 \ud835\udc41\ud835\udc51 \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udf0e(\u210e\ud835\udc51\u00b7 \u210e\ud835\udc57) + \u2211\ufe01 \ud835\udc5c\u2208N\u2212 \ud835\udc51 \ud835\udc5b\ud835\udc5c \ud835\udc41\ud835\udc51 \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udf0e(\u2212\u210e\ud835\udc51\u00b7 \u210e\ud835\udc5c) (7) where \ud835\udc5b\ud835\udc57is the appearance frequency of the positive token \ud835\udc57in the corpus, \ud835\udc5b\ud835\udc5cis the appearance frequency of the negative token \ud835\udc5cin the corpus. To optimize the equation 6, we consider \ud835\udc65= \u210e\ud835\udc51\u00b7 \u210e\ud835\udc57and yield the its partial derivative with \ud835\udc65after explicitly representing the WWW \u201923, May 1\u20135, 2023, Austin, TX, USA Trovato and Tobin, et al. expectation term with equation 7: \ud835\udf15\u2113 \ud835\udf15\ud835\udc65= \ud835\udc5b\ud835\udc51,\ud835\udc57\ud835\udf0e(\u2212\ud835\udc65) + \ud835\udc58\u00b7 \ud835\udc5b\ud835\udc57 \ud835\udc41\ud835\udc51 \ud835\udf0e(\ud835\udc65) (8) where \ud835\udc5b\ud835\udc57is the appearance frequency of the positive token \ud835\udc57in the corpus. We set the partial derivative to zero, and achieve the following maximum point: \u210e\ud835\udc51\u00b7 \u210e\ud835\udc57= \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc5b\ud835\udc51,\ud835\udc57+ \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc41\ud835\udc51 \ud835\udc5b\ud835\udc57 \u2212\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc58 (9) We can see that the optimized \u210e\ud835\udc51,\u210e\ud835\udc57aims to reconstruct the shifted log appearance frequency of token \ud835\udc57in document \ud835\udc56, that is regular- ized by the inverse document frequency (IDF): \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc41\ud835\udc51 \ud835\udc5b\ud835\udc57. For the equation 3, we derive its upper bound as: L\ud835\udc37\ud835\udc45\ud835\udc34\u2264\u22121 \ud835\udc41\ud835\udc51 \ud835\udc41\ud835\udc51 \u2211\ufe01 \ud835\udc51=1 \ud835\udc59\ud835\udc5c\ud835\udc54( \u00cd 0<\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57\u2212\u02c6\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57\ud835\udc52\u210e\ud835\udc56\u00b7\u210e\ud835\udc57 \u00cd\ud835\udc41\ud835\udc54 \ud835\udc5c=1 \ud835\udc52\u210e\ud835\udc56\u00b7\u210e\ud835\udc5c ) (10) where \ud835\udc41\ud835\udc54is the number of nodes in the citation graph (including positive and negative nodes). According to the Jensen\u2019s inequal- ity [9], we further rewrite the equation 10 as: L\ud835\udc37\ud835\udc45\ud835\udc34\u2264\u22121 \ud835\udc41\ud835\udc51 \ud835\udc41\ud835\udc51 \u2211\ufe01 \ud835\udc51=1 \u2212\u02c6\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57\ud835\udc59\ud835\udc5c\ud835\udc54( \u00cd 0<\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57\ud835\udc52\u210e\ud835\udc56\u00b7\u210e\ud835\udc57 \u00cd\ud835\udc41\ud835\udc54 \ud835\udc5c=1 \ud835\udc52\u210e\ud835\udc56\u00b7\u210e\ud835\udc5c ) (11) We approximate the upper bound with the negative sampling: max 1 \ud835\udc41\ud835\udc51 \ud835\udc41\ud835\udc51 \u2211\ufe01 \ud835\udc51=1 \u2211\ufe01 0<\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57 \u2212\u02c6\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57\ud835\udc59\ud835\udc5c\ud835\udc54(\ud835\udf0e(\u210e\ud835\udc56\u00b7 \u210e\ud835\udc57))+ \ud835\udc58\u00b7 E\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc5c=0\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udf0e(\u2212\u210e\ud835\udc56\u00b7 \u210e\ud835\udc5c) (12) It has similar formulation with the equation 6, thus we yield its optimized point as following: \u210e\ud835\udc56\u00b7 \u210e\ud835\udc57= \ud835\udc59\ud835\udc5c\ud835\udc54\u2212\u02c6\ud835\udc34\u2032\ud835\udc51 \ud835\udc56,\ud835\udc57+ \ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc41\ud835\udc51 \ud835\udc5b+ \ud835\udc51 \u2212\ud835\udc59\ud835\udc5c\ud835\udc54\ud835\udc58 (13) where \ud835\udc5b+ \ud835\udc51is the number of neighbours of document \ud835\udc51. We can find that optimizing the L\ud835\udc37\ud835\udc45\ud835\udc34loss implicitly refers to factorizing the shifted log weighted citation matrix \ud835\udc34\u2032\ud835\udc51. The implicitly reconstructing of the log citation graph \ud835\udc34\u2032\ud835\udc51is similar to the graph auto-encoder [14], which can learn representa- tions of source documents efficiently via capturing the topological structure information in the citation graph. The log appearance frequency of each token is similar to the document topic modeling process [4, 35, 37], thus capturing the global context semantics of documents. The analysis helps to explain why and how the designed hierarchical graph contrastive learning on improving representa- tions of source documents from the perspective of reconstructing document contents and their correlation structure. 4.4 Decoder We use the standard transformer [30] based decoder similar to pre- vious methods [19, 28]. We feed token representations of document \ud835\udc51and its references, along with previously generated tokens \ud835\udc66\ud835\udc51 \ud835\udc61\u22121 to get the current output: \u210e\ud835\udc51 \ud835\udc61= LN(\ud835\udc66\ud835\udc51 \ud835\udc61\u22121 + SELFATTN(\ud835\udc66\ud835\udc51 \ud835\udc61\u22121)) \u210e\ud835\udc51 \ud835\udc61= LN(\u210e\ud835\udc51 \ud835\udc61+ CROSSATTN(\u210e\ud835\udc51 \ud835\udc61, \u02c6\u210e\ud835\udc51)) \u210e\ud835\udc51 \ud835\udc61= LN(\u210e\ud835\udc51 \ud835\udc61+ FEEDFORWARD(\u210e\ud835\udc51 \ud835\udc61)) (14) where \u02c6\u210e\ud835\udc51is the set of token representations of document \ud835\udc51and its references based on the graph contrastive learning, LN means layer normalization, and \u210e\ud835\udc51 \ud835\udc61is the hidden representation of the current token in the decoder. The final training loss is: L = \u22121 \ud835\udc41\ud835\udc51 \ud835\udc41\ud835\udc51 \u2211\ufe01 \ud835\udc51=1 \ud835\udc5b \u2211\ufe01 \ud835\udc61=1 \ud835\udc5d(\ud835\udc66\ud835\udc51 \ud835\udc61| \u02c6\ud835\udc66\ud835\udc51 <\ud835\udc61) + \ud835\udefcL\ud835\udc37\ud835\udc45\ud835\udc34+ \ud835\udefdL\ud835\udc47\ud835\udc45\ud835\udc34 (15) where \ud835\udefc, \ud835\udefdare parameters to control the effect of graph contrastive learning, and the first term is the negative conditional log-likelihood of the target token \ud835\udc66\ud835\udc51 \ud835\udc61. 5",
  "conclusion": "We propose a novel self-supervised framework for the summariza- tion of scientific papers based on the citation graph and contrastive learning, to make better use of references and citation correlations. We also propose a novel biomedical domain-specific dataset, by which we expect to support evaluation and method development in the research community. Experimental results on two benchmark datasets show the effectiveness of our proposed method. There are several limitations that could be addressed in the future: 1) For the high-quality content selection of references, we only utilize the ROUGE score as the semantic similarity metric and evaluation met- ric. Other advanced metrics such as the BERTScore can be explored in the future. 2) The PLM encoder in our model only encodes a limited number of input tokens. However, the average document length of documents is up to four thousand. It is expected to address this issue in the future. 3) We only utilize the document contents of source documents and their references. There is rich structural information for scientific documents, such as title, introduction, and related work, that can be incorporated further. ACKNOWLEDGMENTS We would like to thank the Computational Shared Facility at The University of Manchester for offering computing resources. We also want to thank Jimin Huang and Paul Thompson for their CitationSum: Citation-aware Graph Contrastive Learning for Scientific Paper Summarization WWW \u201923, May 1\u20135, 2023, Austin, TX, USA suggestions and feedback on this paper. This research is supported by the Biotechnology and Biological Sciences Research Council (BBSRC), BB/P025684/1, the Joint Laboratory on Credit Technology, the National Key Research and Development Program of China (No.2021ZD0113304), and Application Foundation Frontier Project of Wuhan (Grant NO.2020010601012168)."
}